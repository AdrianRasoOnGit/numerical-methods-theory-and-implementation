# Aitken's method

The Aitken method is quite particular compared to the three previous methods (bisection, secant, Newton), since it is not a specific technique used to tackle problems, but rather a highly valuable complement that allows one to accelerate the convergence of numerical methods. In this sense, it would be more appropriate to speak of Aitken's acceleration, but we will call it a method to keep the naming consistent with the inventory structure followed in the theoretical block of the project.

Aitken's acceleration accompanies other methods by taking a sequence that is indeed converging (for example, and this will usually be the case, one obtained after applying Newton's method a couple of times) and transforming it into a sequence that converges more quickly. The Aitken method is designed to improve sequences generated by fixed-point iterations (next method to introduce), such as Newton's method, although it can also be used in the case of the secant method. Something to bear in mind is that, if Newton's is used in ideal conditions, and converges quadratically, Aitken won't improve the performance. It is a support for linear convergent applications!

## Conditions

In order to use Aitken's method, we require:

- C1: A sequence ${x_n}$ that converges linearly to a limit $L$.
- C2: Three consecutive values $x_n$, $x_{n+1}$, and $x_{n+2}$ can be computed.
- C3: The second forward difference

```math
\Delta^2 x_n = x_{n+2} - 2x_{n+1} + x_n
```
does not vanish, so the formula is well-defined.

The conditions presented here are key. One fascinating condition is that we are only asked to provide just three values of the sequence, which leaves some room to tinker the behavior of both the chosen method and the acceleration.

## Algorithm

Let's assume we have a linearly convergent sequence ${x_n}$, and we begin forming the first and second forward differences:

```math
\Delta x_n = x_{n+1} - x_n,
\Delta^2 x_n = x_{n+2} + x_n.
```

Provided that $\Delta^2 x_n \neq 0$, Aitken's accelerated value is given by the following formula:

```math
\hat{x}_n = x_n - (Δ x_n)^2 / (Δ^2 x_n)
```

The formula generates a new approximation $\hat{x}_n$ that usually faster, due to the true limit. In this part of the algorithm is where we are transforming our convergence rate from a linear regime, to a quadratic regime, that is, Newton's method ideal performance.

The iterations will continue until the accelerated values meet the tolerance:

```math
|\hat{x}_n - x_n| < \epsilon
```

Once the tolerance is reached, we adopt $\hat{x}_n$ as the approximated root of the equation.
